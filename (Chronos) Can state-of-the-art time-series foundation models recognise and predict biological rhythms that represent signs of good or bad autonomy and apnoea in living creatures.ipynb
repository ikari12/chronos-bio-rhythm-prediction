{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecb342ac-c6ad-4190-80b2-9e6ed25d48d8",
   "metadata": {},
   "source": [
    "# Building a computing environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa55a4b-1cb1-448c-bb59-73a363bf7acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm ipywidgets joblib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5634b4ce-219e-4c36-a7ec-242febe843a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wfdb neurokit2 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d030ab-eb9a-4e02-a760-e5758f012beb",
   "metadata": {},
   "source": [
    "# Checking ECG test data on a stand-alone basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd63631-a48c-4ecc-b268-2356242ca40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # Used for generating the time axis\n",
    "import os\n",
    "\n",
    "# --- Settings ---\n",
    "record_name = 'a01'  # Record name to load (e.g., 'a01', 'c01', 'x01')\n",
    "database_name = 'apnea-ecg'\n",
    "db_version = '1.0.0' # Database version\n",
    "pn_dir_path = f'{database_name}/{db_version}' # PhysioNet directory path\n",
    "\n",
    "# Data storage directory (if not specified, uses wfdb default location)\n",
    "# The wfdb library caches data, so it doesn't download every time.\n",
    "# save_dir = 'wfdb_data'\n",
    "# if not os.path.exists(save_dir):\n",
    "#     os.makedirs(save_dir)\n",
    "\n",
    "print(f\"Reading record: {record_name} from database: {database_name}\")\n",
    "\n",
    "# --- Read header information and signal data ---\n",
    "try:\n",
    "    # Read directly from PhysioNet (will attempt download if not found locally)\n",
    "    # Specify the range to load using sampfrom, sampto (full data is large, so loading the first 10 mins = 600s * 100Hz = 60000 samples here)\n",
    "    # Load in physical units (mV) using physical=True\n",
    "    record = wfdb.rdrecord(record_name, pn_dir=pn_dir_path, sampfrom=0, sampto=60000, physical=True)\n",
    "\n",
    "    # Get the loaded signal data (p_signal contains physical units)\n",
    "    signal = record.p_signal\n",
    "    # Get header information\n",
    "    fs = record.fs\n",
    "    sig_name = record.sig_name\n",
    "    units = record.units\n",
    "    record_len_samples = record.sig_len # This indicates the total length of the file\n",
    "\n",
    "    print(f\"\\n--- Record Header Info ---\")\n",
    "    print(f\"Sampling frequency: {fs} Hz\")\n",
    "    print(f\"Signal names: {sig_name}\")\n",
    "    print(f\"Signal units: {units}\")\n",
    "    print(f\"Total record length: {record_len_samples} samples ({record_len_samples / fs / 3600:.1f} hours)\")\n",
    "    print(f\"Loaded signal shape: {signal.shape}\") # Shape of the loaded segment\n",
    "    print(\"-\" * 25)\n",
    "\n",
    "    # --- Read apnoea annotations ---\n",
    "    # Specify the apnoea annotation file using extension='apn'\n",
    "    annotation = wfdb.rdann(record_name, extension='apn', pn_dir=pn_dir_path, sampfrom=0, sampto=60000)\n",
    "\n",
    "    # Display annotation information (first few within the loaded range)\n",
    "    print(\"--- Apnea Annotations (in loaded range, first 5) ---\")\n",
    "    print(f\"Annotation samples: {annotation.sample[:5]}\") # Sample numbers where annotations occur\n",
    "    print(f\"Annotation symbols: {annotation.symbol[:5]}\") # Annotation symbols ('N': Normal breathing, 'A': Apnoea)\n",
    "    print(f\"Number of annotations in loaded range: {len(annotation.sample)}\")\n",
    "    print(\"-\" * 25)\n",
    "\n",
    "    # --- Simple plot (first 30 seconds) ---\n",
    "    plot_duration_sec = 30\n",
    "    plot_samples = int(plot_duration_sec * fs)\n",
    "\n",
    "    # Adjust if the loaded signal is shorter than the plot range\n",
    "    if plot_samples > signal.shape[0]:\n",
    "        plot_samples = signal.shape[0]\n",
    "        plot_duration_sec = plot_samples / fs\n",
    "\n",
    "    if plot_samples > 0: # Only plot if data was loaded\n",
    "        # Create the time axis (using NumPy)\n",
    "        time_axis = np.arange(plot_samples) / fs\n",
    "\n",
    "        plt.figure(figsize=(15, 4))\n",
    "        # Apnea-ECG has one signal (ECG), access via signal[:, 0]\n",
    "        plt.plot(time_axis, signal[:plot_samples, 0], label=f'{sig_name[0]} ({units[0]})')\n",
    "        plt.title(f'ECG Signal for Record {record_name} (First {plot_duration_sec:.1f} seconds)')\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel(f'Amplitude ({units[0]})')\n",
    "\n",
    "        # Plot annotations (only within the plot range)\n",
    "        # Note: Annotations are provided per minute\n",
    "        ann_indices_in_plot = [i for i, s in enumerate(annotation.sample) if s < plot_samples]\n",
    "        if ann_indices_in_plot:\n",
    "            ann_times = annotation.sample[ann_indices_in_plot] / fs\n",
    "            ann_symbols = [annotation.symbol[i] for i in ann_indices_in_plot]\n",
    "            # Draw vertical lines and symbols at annotation positions\n",
    "            y_min, y_max = plt.ylim() # Get the current y-axis limits\n",
    "            # Ensure legend handles are created only once\n",
    "            has_legend_handle = 'Apnea Annotation' in plt.gca().get_legend_handles_labels()[1]\n",
    "            for t, s in zip(ann_times, ann_symbols):\n",
    "                # Vertical line at annotation position\n",
    "                plt.axvline(t, color='red', linestyle='--', alpha=0.7, label='Apnea Annotation' if not has_legend_handle else \"\")\n",
    "                has_legend_handle = True # Set flag after first use\n",
    "                # Display annotation symbol near the top of the plot\n",
    "                plt.text(t, y_max * 0.95, s, color='red', fontsize=12, ha='center', va='top')\n",
    "\n",
    "        plt.grid(True)\n",
    "        # Only show legend if annotation lines were added\n",
    "        if 'Apnea Annotation' in plt.gca().get_legend_handles_labels()[1]:\n",
    "             plt.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No signal data loaded to plot.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError reading record {record_name} from {database_name}: {e}\")\n",
    "    print(\"Please ensure the 'wfdb' library is installed ('pip install wfdb').\")\n",
    "    print(\"An internet connection may be required for the first download.\")\n",
    "    print(f\"Check if the database '{database_name}' and record '{record_name}' exist on PhysioNet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e806ad-4ba8-40e1-8454-ab971d254a9c",
   "metadata": {},
   "source": [
    "# Creating RRIs to feed into the Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d968d-306d-4586-bf70-2ffe3c0cc3e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import neurokit2 as nk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Import joblib for parallel processing\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm # tqdm for progress bar\n",
    "import time\n",
    "import os\n",
    "import platform # To get os.cpu_count()\n",
    "\n",
    "# --- Configuration ---\n",
    "database_name = 'apnea-ecg'\n",
    "db_version = '1.0.0'\n",
    "pn_dir_path = f'{database_name}/{db_version}'\n",
    "output_dir = 'apnea_ecg_rri_files'  # Directory to save the RRI files\n",
    "target_fs = 100  # Target sampling frequency (Hz) for RRI calculation\n",
    "\n",
    "# Number of parallel workers (adjust according to CPU cores)\n",
    "# Using -1 tells joblib to use all available cores, -2 uses all but one.\n",
    "# Be mindful of memory consumption. Let's keep the manual setting option too.\n",
    "# num_workers = -2 # Use all cores but one (joblib recommendation)\n",
    "num_workers = os.cpu_count() - 1 if os.cpu_count() and os.cpu_count() > 1 else 1\n",
    "# num_workers = 4 # Manual override as in the previous example\n",
    "\n",
    "# Full list of record names (a01-a20, b01-b05, c01-c05, x01-x30)\n",
    "record_names_a = [f'a{i:02d}' for i in range(1, 21)]\n",
    "record_names_b = [f'b{i:02d}' for i in range(1, 6)]\n",
    "record_names_c = [f'c{i:02d}' for i in range(1, 6)]\n",
    "record_names_x = [f'x{i:02d}' for i in range(1, 31)]\n",
    "all_record_names = sorted(record_names_a + record_names_b + record_names_c + record_names_x)\n",
    "\n",
    "print(f\"Target database: {database_name}/{db_version}\")\n",
    "print(f\"Total records to process: {len(all_record_names)}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Target sampling frequency for RRI: {target_fs} Hz\")\n",
    "print(f\"Number of parallel workers (n_jobs for joblib): {num_workers}\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- Function to process a single record and save RRI to a file ---\n",
    "# This function remains the same as before\n",
    "def process_and_save_rri(record_name, db_path, out_dir, fs_target):\n",
    "    \"\"\"\n",
    "    Reads ECG for the specified record, preprocesses, extracts RRI (ms),\n",
    "    and saves it to a CSV file.\n",
    "    Returns: (record_name, success_status, message_or_filepath)\n",
    "    \"\"\"\n",
    "    output_filepath = os.path.join(out_dir, f\"{record_name}_rri.csv\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        # 1. Load data\n",
    "        record = wfdb.rdrecord(record_name, pn_dir=db_path, physical=True)\n",
    "        ecg_signal_original = record.p_signal[:, 0]\n",
    "        fs_original = record.fs\n",
    "\n",
    "        # 2. Resample (if necessary)\n",
    "        if fs_original != fs_target:\n",
    "            ecg_signal = nk.signal_resample(ecg_signal_original,\n",
    "                                           sampling_rate=fs_original,\n",
    "                                           desired_sampling_rate=fs_target,\n",
    "                                           method='interpolation')\n",
    "        else:\n",
    "            ecg_signal = ecg_signal_original\n",
    "        fs = fs_target\n",
    "\n",
    "        # 3. NeuroKit2 for preprocessing and R-peak detection\n",
    "        ecg_cleaned = nk.ecg_clean(ecg_signal, sampling_rate=fs, method=\"neurokit\")\n",
    "        _, rpeaks_info = nk.ecg_peaks(ecg_cleaned, sampling_rate=fs, method=\"neurokit\", correct_artifacts=True)\n",
    "        rpeaks_indices = rpeaks_info['ECG_R_Peaks']\n",
    "\n",
    "        # 4. Calculate RRI series (in milliseconds)\n",
    "        if len(rpeaks_indices) > 1:\n",
    "            rri_series_ms = np.diff(rpeaks_indices) / fs * 1000.0\n",
    "\n",
    "            if len(rri_series_ms) > 0:\n",
    "                # 5. Save to CSV file (no header, no index, single column)\n",
    "                pd.Series(rri_series_ms).to_csv(output_filepath, index=False, header=False)\n",
    "                duration = time.time() - start_time\n",
    "                # Return success status and info message including the file path\n",
    "                return record_name, True, f\"Saved to {output_filepath} ({len(rri_series_ms)} RRIs, {duration:.2f}s)\"\n",
    "            else:\n",
    "                duration = time.time() - start_time\n",
    "                # Return failure status and message\n",
    "                return record_name, False, f\"No valid RRIs calculated after diff ({duration:.2f}s)\"\n",
    "        else:\n",
    "            duration = time.time() - start_time\n",
    "            # Return failure status and message\n",
    "            return record_name, False, f\"Not enough R-peaks found ({len(rpeaks_indices)}) ({duration:.2f}s)\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # Print the exception for debugging during development if needed\n",
    "        print(f\"Exception in process_and_save_rri for {record_name}: {e}\")\n",
    "        duration = time.time() - start_time\n",
    "        # Return failure status and error message\n",
    "        return record_name, False, f\"Error: {e} ({duration:.2f}s)\"\n",
    "\n",
    "# --- Execute parallel processing using joblib ---\n",
    "print(f\"\\nStarting parallel processing with joblib (n_jobs={num_workers})...\")\n",
    "start_total_time = time.time()\n",
    "\n",
    "# Use joblib's Parallel and delayed functions\n",
    "# Wrap the iterable (generator expression) with tqdm for progress display\n",
    "results = Parallel(n_jobs=num_workers)(\n",
    "    delayed(process_and_save_rri)(name, pn_dir_path, output_dir, target_fs)\n",
    "    for name in tqdm(all_record_names, desc=\"Processing Records\")\n",
    ")\n",
    "\n",
    "# 'results' will be a list containing the tuples returned by process_and_save_rri\n",
    "\n",
    "end_total_time = time.time()\n",
    "print(f\"\\n--- Parallel Processing Complete ---\")\n",
    "print(f\"Total time: {end_total_time - start_total_time:.2f} seconds.\")\n",
    "\n",
    "# --- Summarise and display results ---\n",
    "# This part remains the same as it processes the 'results' list\n",
    "success_count = 0\n",
    "failed_records = []\n",
    "# Sort results alphabetically by record name for consistent output\n",
    "for record_name_res, status, message in sorted(results):\n",
    "    if status:\n",
    "        success_count += 1\n",
    "        # Optionally print success message:\n",
    "        # print(f\"[Success] {record_name_res}: {message}\")\n",
    "    else:\n",
    "        failed_records.append((record_name_res, message))\n",
    "        # Print details only for failed records\n",
    "        print(f\"[Failed] {record_name_res}: {message}\")\n",
    "\n",
    "print(f\"\\n--- Summary ---\")\n",
    "print(f\"Successfully processed and saved RRI for {success_count} out of {len(all_record_names)} records.\")\n",
    "if failed_records:\n",
    "    print(f\"Failed to process {len(failed_records)} records.\")\n",
    "    # Optionally list failed records again:\n",
    "    # for name, msg in failed_records:\n",
    "    #     print(f\"  - {name}: {msg}\")\n",
    "print(f\"RRI data saved in directory: '{output_dir}' as individual CSV files (e.g., a01_rri.csv).\")\n",
    "\n",
    "print(\"\\nThese CSV files can now be loaded individually for further analysis or model input preparation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cfa117-b5c6-4869-924e-99cdf1c2faa4",
   "metadata": {},
   "source": [
    "# Submitting the created RRIs to Chronos to obtain forecast results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aaf8a8-0069-4c6e-b919-f66da5ce0e06",
   "metadata": {},
   "source": [
    "## Install missing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c85478-8194-4dd4-b184-16097cbbc6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas matplotlib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c932b6-398a-4267-921d-12b7decf5745",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gluonts torch transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f7c3c9-fc29-4a20-9782-2e2063bd4e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/amazon-science/chronos-forecasting.git --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d09ab-bf15-4912-bc62-261f51c6aa9a",
   "metadata": {},
   "source": [
    "## Forecasting with Chronos and storing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8094e70-e480-431a-a7ca-55397552296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Part 1 Final (Revised based on user's code + necessary fixes + NPY saving) ===\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "# import matplotlib.pyplot as plt # Plotting removed for focus on saving\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "# Use tqdm.notebook if in Jupyter/Colab, otherwise use standard tqdm\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Ignore specific warnings if they become noisy (optional)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "# Attempt to import the ChronosPipeline\n",
    "try:\n",
    "    # This import depends on the chronos-ts package structure\n",
    "    from chronos import ChronosPipeline\n",
    "except ImportError:\n",
    "    print(\"ERROR: Could not import ChronosPipeline.\")\n",
    "    print(\"Please ensure you have installed the required package, likely via:\")\n",
    "    print(\"pip install git+https://github.com/amazon-science/chronos-forecasting.git\") # User's suggestion kept\n",
    "    exit()\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "original_rri_dir = 'apnea_ecg_rri_files'    # Directory with original RRI CSV files\n",
    "predicted_npy_dir = 'apnea_ecg_rri_predicted_samples_chronos_npy' # Directory for Chronos prediction SAMPLES (.npy)\n",
    "os.makedirs(predicted_npy_dir, exist_ok=True) # Create output directory for predictions\n",
    "\n",
    "# Set prediction horizon (e.g., 1 hour for practicality)\n",
    "prediction_horizon_hours = 1\n",
    "# prediction_horizon_hours = 6 # Or uncomment this to try 6 hours (very long compute & large files)\n",
    "\n",
    "context_hours = 1            # History length (e.g., 1 hour)\n",
    "num_prediction_samples = 100 # Number of sample paths to generate for probabilistic forecast\n",
    "\n",
    "# Choose the Chronos model size\n",
    "model_checkpoint = \"amazon/chronos-t5-small\" # Smallest model for testing\n",
    "# model_checkpoint = \"amazon/chronos-t5-base\" # Larger alternative\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Use bfloat16 for potential speed/memory improvements if GPU supports it\n",
    "torch_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float32\n",
    "\n",
    "print(f\"--- Chronos Prediction Sample Generation ---\")\n",
    "print(f\"Using Chronos model: {model_checkpoint}\")\n",
    "print(f\"Using device: {device}, dtype: {torch_dtype}\")\n",
    "print(f\"Prediction horizon: {prediction_horizon_hours} hours\")\n",
    "print(f\"Context window: {context_hours} hours\")\n",
    "print(f\"Number of forecast samples: {num_prediction_samples}\")\n",
    "print(f\"Loading original RRI from: {original_rri_dir}\")\n",
    "print(f\"Saving predicted RRI samples to: {predicted_npy_dir} (as .npy files)\")\n",
    "\n",
    "# Get all original RRI file paths\n",
    "all_original_rri_files = sorted(glob.glob(os.path.join(original_rri_dir, \"*_rri.csv\")))\n",
    "if not all_original_rri_files:\n",
    "    raise FileNotFoundError(f\"No original RRI CSV files found in '{original_rri_dir}'. Run the RRI extraction script first.\")\n",
    "print(f\"Found {len(all_original_rri_files)} original RRI files to process.\")\n",
    "\n",
    "# --- 2. Load Chronos Pipeline (once) ---\n",
    "print(f\"\\nLoading Chronos pipeline for {model_checkpoint}...\")\n",
    "pipeline = None # Initialize pipeline variable\n",
    "try:\n",
    "    # Load the pipeline first. We will explicitly move it later.\n",
    "    pipeline = ChronosPipeline.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch_dtype,\n",
    "    )\n",
    "    # Optional: Check the device of a parameter to confirm\n",
    "    print(f\"Device of first param: {next(pipeline.model.parameters()).device}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError loading or moving Chronos pipeline: {e}\")\n",
    "    print(\"Please ensure 'chronos-ts', 'torch', 'transformers', and 'accelerate' are installed correctly.\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. Function to predict samples and save as .npy (with dtype/device fixes) ---\n",
    "def predict_and_save_samples_chronos_final(rri_filepath, context_h, pred_h, pipeline_instance, pred_out_dir, num_samples, target_dtype, target_device):\n",
    "    \"\"\"\n",
    "    Loads RRI, predicts samples using ChronosPipeline, saves ALL samples as .npy file.\n",
    "    Includes fixes for previously observed dtype and device errors.\n",
    "    Returns: (record_name, success_status, message)\n",
    "    \"\"\"\n",
    "    record_name = os.path.basename(rri_filepath).replace('_rri.csv', '')\n",
    "    output_filepath = os.path.join(pred_out_dir, f\"{record_name}_rri_predicted_samples.npy\")\n",
    "    start_time = time.time()\n",
    "    status = False\n",
    "    message = \"Prediction started\"\n",
    "    processed_shape = None\n",
    "\n",
    "    # Make sure the pipeline exists and is usable\n",
    "    if pipeline_instance is None:\n",
    "         return record_name, False, \"Skipped: Pipeline object is not available.\"\n",
    "\n",
    "    try:\n",
    "        # 1. Load original RRI\n",
    "        rri_series_df = pd.read_csv(rri_filepath, header=None, names=['rri_ms'])\n",
    "        rri_history = rri_series_df['rri_ms'].dropna().values\n",
    "        min_hist_len = 10\n",
    "        if len(rri_history) < min_hist_len:\n",
    "             return record_name, False, f\"Skipped: Insufficient history data ({len(rri_history)} < {min_hist_len})\"\n",
    "\n",
    "        # 2. Calculate context/prediction length\n",
    "        mean_rri = np.mean(rri_history)\n",
    "        avg_hr_bpm = 60 / (mean_rri / 1000) if mean_rri > 0 else 75\n",
    "        # Calculate desired prediction length in samples\n",
    "        pred_len_calculated = max(1, int(pred_h * 3600 * avg_hr_bpm / 60))\n",
    "        ctx_len = max(1, int(context_h * 3600 * avg_hr_bpm / 60))\n",
    "\n",
    "        # Ensure context length is valid\n",
    "        if ctx_len >= len(rri_history):\n",
    "            print(f\"Warning for {record_name}: Requested context length ({ctx_len}) >= history length ({len(rri_history)}). Using history length - 1.\")\n",
    "            ctx_len = len(rri_history) - 1\n",
    "        if ctx_len <= 0:\n",
    "             return record_name, False, f\"Skipped: Not enough history ({len(rri_history)}) for context_length calculation (needed > 0)\"\n",
    "\n",
    "        # 3. Prepare input context tensor (Applying dtype and device fixes)\n",
    "        context_rri = rri_history[-ctx_len:]\n",
    "        # Create tensor with the correct dtype determined during setup\n",
    "        # Ensure input is float, even if original RRI was int\n",
    "        context = torch.tensor(context_rri, dtype=torch.float32).to(target_dtype)\n",
    "        # Explicitly move the tensor to the target device\n",
    "        #context = context.to(target_device)\n",
    "        # Add batch dimension: [ctx_len] -> [1, ctx_len]\n",
    "        context = context.unsqueeze(0)\n",
    "\n",
    "        # 4. Make prediction (generate samples) using the pipeline\n",
    "        # Use the *calculated* prediction length here\n",
    "        # NOTE: The previous code had a hardcoded '64'. Using the calculated length now.\n",
    "        # If '64' was intentional, change pred_len_calculated below back to 64.\n",
    "        forecast = pipeline_instance.predict(\n",
    "            context,\n",
    "            pred_len_calculated, # Use calculated prediction length\n",
    "            num_samples=num_samples,\n",
    "        )\n",
    "        # Expected forecast shape: [1 (batch_size), num_samples, pred_len]\n",
    "\n",
    "        if forecast is not None and forecast.ndim == 3 and \\\n",
    "           forecast.shape[0] == 1 and forecast.shape[1] == num_samples and forecast.shape[2] == pred_len_calculated:\n",
    "\n",
    "            # Extract samples [num_samples, pred_len], move to CPU, convert to numpy\n",
    "            # Squeeze the batch dimension before moving/converting\n",
    "            prediction_samples = forecast.squeeze(0).cpu().numpy()\n",
    "\n",
    "            # 5. Save ALL samples as a NumPy binary file (.npy)\n",
    "            np.save(output_filepath, prediction_samples) # Save the 2D array\n",
    "            status = True\n",
    "            processed_shape = prediction_samples.shape\n",
    "            duration = time.time() - start_time\n",
    "            message = f\"Saved prediction samples {processed_shape} to {output_filepath} ({duration:.2f}s)\"\n",
    "        else:\n",
    "            duration = time.time() - start_time\n",
    "            forecast_shape_str = str(getattr(forecast, 'shape', 'None')) # Safer shape printing\n",
    "            message = f\"Pipeline prediction failed or returned unexpected shape: {forecast_shape_str}. Expected: (1, {num_samples}, {pred_len_calculated}) ({duration:.2f}s)\"\n",
    "\n",
    "    except Exception as e:\n",
    "        duration = time.time() - start_time\n",
    "        # Improve error message for device issues\n",
    "        err_msg = f\"Error during prediction/saving for {record_name}: {type(e).__name__} - {e} ({duration:.2f}s)\"\n",
    "        if \"Expected all tensors to be on the same device\" in str(e):\n",
    "            err_msg += f\" [Device Check: Input Context on {context.device if 'context' in locals() else 'N/A'}, Pipeline expected on {target_device}]\"\n",
    "        message = err_msg\n",
    "        status = False # Ensure status is False on exception\n",
    "\n",
    "    return record_name, status, message\n",
    "\n",
    "# --- 4. Loop through all files and predict (SEQUENTIAL execution) ---\n",
    "print(f\"\\nStarting prediction loop for {len(all_original_rri_files)} records (SEQUENTIAL using ChronosPipeline)...\")\n",
    "prediction_results_chronos = []\n",
    "start_pred_loop_time = time.time()\n",
    "\n",
    "# Ensure the pipeline was loaded successfully before proceeding\n",
    "if pipeline is None:\n",
    "    print(\"ERROR: Chronos pipeline failed to load. Cannot start prediction loop.\")\n",
    "else:\n",
    "    for fpath in tqdm(all_original_rri_files, desc=\"Predicting Samples with Chronos\"):\n",
    "        # Call the corrected function, passing torch_dtype and device\n",
    "        res_name, res_status, res_msg = predict_and_save_samples_chronos_final(\n",
    "            fpath, context_hours, prediction_horizon_hours, pipeline, predicted_npy_dir, num_prediction_samples, torch_dtype, device\n",
    "        )\n",
    "        prediction_results_chronos.append((res_name, res_status, res_msg))\n",
    "        # Print status immediately for monitoring progress during long runs\n",
    "        print(f\"  Processed {res_name}: {'OK' if res_status else 'FAIL'} - {res_msg}\")\n",
    "\n",
    "\n",
    "    end_pred_loop_time = time.time()\n",
    "    print(f\"\\n--- Chronos Prediction Loop Finished ---\")\n",
    "    print(f\"Total prediction time: {end_pred_loop_time - start_pred_loop_time:.2f} seconds.\")\n",
    "\n",
    "    # --- 5. Summarise prediction results ---\n",
    "    pred_success_count = sum(1 for _, status, _ in prediction_results_chronos if status)\n",
    "    pred_failed_records = [(name, msg) for name, status, msg in prediction_results_chronos if not status]\n",
    "\n",
    "    print(f\"\\nSuccessfully generated and saved Chronos prediction samples for {pred_success_count} out of {len(all_original_rri_files)} records.\")\n",
    "    if pred_failed_records:\n",
    "        print(f\"Failed to generate/save Chronos prediction samples for {len(pred_failed_records)} records:\")\n",
    "        # Optionally print details for failures (might be long, print first few)\n",
    "        for i, (name, msg) in enumerate(pred_failed_records):\n",
    "             print(f\"  - {name}: {msg}\")\n",
    "             if i > 5: # Limit printing excessive errors\n",
    "                 print(\"  - ... (further errors omitted)\")\n",
    "                 break\n",
    "    print(f\"\\nPredicted RRI samples saved in directory: '{predicted_npy_dir}' as individual .npy files (e.g., a01_rri_predicted_samples.npy).\")\n",
    "    print(\"\\nYou can now proceed to the HRV analysis (Part 2) using these .npy files.\")\n",
    "    print(\"REMEMBER: The HRV analysis code (Part 2) needs to load these .npy files and process the samples (e.g., calculate HRV per sample, then take median).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915bb20e-1ea6-4fd2-96b0-cc4fa2feaf3c",
   "metadata": {},
   "source": [
    "## Statistical tests for the original RRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bc3265-1432-4cc5-905e-71f535a3f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HRV Analysis and Comparison: Original (CSV) vs Predicted (NPY) [Parallelized] ===\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neurokit2 as nk\n",
    "from scipy.stats import mannwhitneyu\n",
    "import os\n",
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "original_rri_dir = 'apnea_ecg_rri_files' # Directory with original RRI CSV files\n",
    "\n",
    "# Define record names for groups\n",
    "record_names_low_ahi = [f'c{i:02d}' for i in range(1, 6)] + [f'x{i:02d}' for i in range(1, 31)] \n",
    "record_names_high_ahi = [f'a{i:02d}' for i in range(1, 21)] + [f'b{i:02d}' for i in range(1, 6)]\n",
    "\n",
    "# Metrics to calculate and compare (using NeuroKit2 column names internally)\n",
    "hrv_metrics_nk_names = ['HRV_MeanNN', 'HRV_SDNN', 'HRV_RMSSD', 'HRV_DFA_alpha1']\n",
    "\n",
    "# Mapping to user-friendly names for reporting results\n",
    "metrics_display_map = {\n",
    "    'HRV_MeanNN': 'Mean RRI (ms)',\n",
    "    'HRV_SDNN': 'SDNN (ms)',\n",
    "    'HRV_RMSSD': 'RMSSD (ms)',\n",
    "    'HRV_DFA_alpha1': 'DFA Alpha 1'\n",
    "}\n",
    "\n",
    "# Number of parallel jobs (-1 uses all available cores)\n",
    "N_JOBS = -1\n",
    "\n",
    "# --- Ignore Warnings ---\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning, module='neurokit2.*')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='pandas.*')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# --- HRV Calculation Function (No change needed here) ---\n",
    "def calculate_hrv_metrics(rri_ms, metrics_to_extract):\n",
    "    \"\"\"Calculates specified HRV metrics using NeuroKit2.\"\"\"\n",
    "    if not isinstance(rri_ms, np.ndarray):\n",
    "        rri_ms = np.array(rri_ms)\n",
    "    if rri_ms is None or len(rri_ms) < 50:\n",
    "        return None\n",
    "    if np.any(rri_ms <= 0) or np.nanstd(rri_ms) == 0:\n",
    "        return None\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            # Calculate standard HRV indices. NeuroKit2 efficiently calculates many at once.\n",
    "            rri_ms = nk.intervals_to_peaks(rri_ms.flatten(), sampling_rate=100)\n",
    "            hrv_indices = nk.hrv(rri_ms, sampling_rate=100) # sampling_rate is mainly for frequency domain bands\n",
    "        # Combine results\n",
    "        # Use .get() on the DataFrame returned, accessing the first row ([0])\n",
    "        print(hrv_indices)\n",
    "        metrics = {}\n",
    "        if 'HRV_MeanNN' in metrics_to_extract:\n",
    "            metrics['HRV_MeanNN'] = hrv_indices.get('HRV_MeanNN', pd.Series([np.nan])).iloc[0]\n",
    "        if 'HRV_SDNN' in metrics_to_extract:\n",
    "             metrics['HRV_SDNN'] = hrv_indices.get('HRV_SDNN', pd.Series([np.nan])).iloc[0]\n",
    "        if 'HRV_RMSSD' in metrics_to_extract:\n",
    "             metrics['HRV_RMSSD'] = hrv_indices.get('HRV_RMSSD', pd.Series([np.nan])).iloc[0]\n",
    "        if 'HRV_DFA_alpha1' in metrics_to_extract:\n",
    "             metrics['HRV_DFA_alpha1'] = hrv_indices.get('HRV_DFA_alpha1', pd.Series([np.nan])).iloc[0]\n",
    "\n",
    "        # Filter only requested metrics that were successfully calculated\n",
    "        final_metrics = {k: v for k, v in metrics.items() if k in metrics_to_extract and not pd.isna(v)}\n",
    "\n",
    "        # Return None if no valid metrics were calculated\n",
    "        return final_metrics if final_metrics else None\n",
    "\n",
    "    except Exception as e:\n",
    "        # Optional: More specific error logging if needed\n",
    "        print(f\" Debug: HRV calculation error for sample: {type(e).__name__} - {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Worker Function for CSV Processing ---\n",
    "def process_single_csv_file(fpath, metrics_to_extract):\n",
    "    \"\"\"Processes a single CSV file to calculate HRV metrics.\"\"\"\n",
    "    record_name = os.path.basename(fpath).replace('_rri.csv', '')\n",
    "    try:\n",
    "        rri_df = pd.read_csv(fpath, header=None, names=['rri_ms'])\n",
    "        rri_ms = rri_df['rri_ms'].dropna().values\n",
    "        metrics = calculate_hrv_metrics(rri_ms, metrics_to_extract)\n",
    "        if metrics is not None:\n",
    "            return record_name, metrics\n",
    "        else:\n",
    "            # print(f\"  Skipping {record_name} (CSV): Could not calculate valid metrics.\") # Optional: uncomment for detailed skipping info\n",
    "            return record_name, None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  Warning: File not found {fpath}\")\n",
    "        return record_name, None\n",
    "    except Exception as e:\n",
    "        print(f\"  Skipping {record_name} (CSV): Error during processing - {type(e).__name__}\")\n",
    "        return record_name, None\n",
    "\n",
    "# --- Function to Analyze Group HRV from CSV files (Parallelized) ---\n",
    "def analyze_group_hrv_csv_parallel(file_list, group_label, metrics_to_extract, n_jobs):\n",
    "    \"\"\"Loads original RRI CSV files, calculates HRV in parallel, returns DataFrame.\"\"\"\n",
    "    print(f\"\\nAnalyzing HRV for group: {group_label} (Original CSV data) using {n_jobs if n_jobs > 0 else 'all'} cores\")\n",
    "    hrv_results = []\n",
    "    record_names_processed = []\n",
    "    if not file_list:\n",
    "        print(f\"  No CSV files found for group {group_label}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Use joblib to parallelize the processing of each file\n",
    "    # verbose=10 provides progress updates\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=10)(\n",
    "        delayed(process_single_csv_file)(fpath, metrics_to_extract) for fpath in file_list\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print(f\"  Parallel processing took {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    # Collect results\n",
    "    for result in results:\n",
    "        if result is not None:\n",
    "            record_name, metrics = result\n",
    "            if metrics is not None:\n",
    "                hrv_results.append(metrics)\n",
    "                record_names_processed.append(record_name)\n",
    "\n",
    "    if hrv_results:\n",
    "        hrv_df = pd.DataFrame(hrv_results, index=record_names_processed)\n",
    "        # Ensure all expected columns exist, fill with NaN if missing entirely\n",
    "        for col in metrics_to_extract:\n",
    "            if col not in hrv_df.columns:\n",
    "                hrv_df[col] = np.nan\n",
    "        # Reorder columns to match original request\n",
    "        hrv_df = hrv_df[metrics_to_extract]\n",
    "        print(f\"  Successfully processed {len(hrv_df)} records for {group_label} (CSV).\")\n",
    "        return hrv_df\n",
    "    else:\n",
    "        print(f\"  No valid HRV results could be calculated for {group_label} (CSV).\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# --- Function to Compare Groups and Perform Statistical Tests (No change needed here) ---\n",
    "def compare_hrv_groups(df_low, df_high, metrics_map, label_suffix=\"\"):\n",
    "    \"\"\"Performs Mann-Whitney U tests for specified metrics between two groups.\"\"\"\n",
    "    print(f\"\\n--- Statistical Comparison: Low AHI vs High AHI {label_suffix} ---\")\n",
    "    if not isinstance(df_low, pd.DataFrame) or df_low.empty or not isinstance(df_high, pd.DataFrame) or df_high.empty:\n",
    "        print(\"Comparison skipped: One or both input dataframes are empty or invalid.\")\n",
    "        return None\n",
    "\n",
    "    comparison_results = {}\n",
    "    print(f\"Comparing Low AHI (n={len(df_low.dropna(how='all'))}) vs High AHI (n={len(df_high.dropna(how='all'))}) using Mann-Whitney U test.\") # Count non-NA rows\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Metric':<18} | {'Low Median (IQR)':<25} | {'High Median (IQR)':<25} | {'p-value':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    full_metrics_map = {**metrics_map, 'Mean HR (bpm)': 'Mean HR (bpm)'}\n",
    "\n",
    "    for nk_name, display_name in full_metrics_map.items():\n",
    "        is_hr = (display_name == 'Mean HR (bpm)')\n",
    "        data_low, data_high = None, None\n",
    "\n",
    "        if is_hr:\n",
    "            mean_rri_col = 'HRV_MeanNN'\n",
    "            if mean_rri_col in df_low.columns and mean_rri_col in df_high.columns:\n",
    "                # Ensure division by zero doesn't happen and results are finite\n",
    "                data_low_hr = (60000 / df_low[mean_rri_col].replace(0, np.nan)).dropna()\n",
    "                data_high_hr = (60000 / df_high[mean_rri_col].replace(0, np.nan)).dropna()\n",
    "                data_low_hr = data_low_hr[np.isfinite(data_low_hr)]\n",
    "                data_high_hr = data_high_hr[np.isfinite(data_high_hr)]\n",
    "                if len(data_low_hr) >= 3 and len(data_high_hr) >= 3:\n",
    "                    data_low = data_low_hr\n",
    "                    data_high = data_high_hr\n",
    "                else:\n",
    "                    print(f\"{display_name:<18} | {'Low n<3' if len(data_low_hr)<3 else 'OK':<25} | {'High n<3' if len(data_high_hr)<3 else 'OK':<25} | {'N/A':<10}\")\n",
    "                    continue # Skip to next metric if not enough data for HR\n",
    "            else:\n",
    "                print(f\"{display_name:<18} | {'Mean RRI missing':<25} | {'Mean RRI missing':<25} | {'N/A':<10}\")\n",
    "                continue # Skip HR calculation if Mean RRI is missing\n",
    "        else:\n",
    "            # Standard Handling for other metrics\n",
    "            if nk_name not in df_low.columns or nk_name not in df_high.columns:\n",
    "                print(f\"{display_name:<18} | {'Metric missing':<25} | {'Metric missing':<25} | {'N/A':<10}\")\n",
    "                continue\n",
    "            data_low_std = df_low[nk_name].dropna()\n",
    "            data_high_std = df_high[nk_name].dropna()\n",
    "            if len(data_low_std) >= 3 and len(data_high_std) >= 3:\n",
    "                 data_low = data_low_std\n",
    "                 data_high = data_high_std\n",
    "            else:\n",
    "                print(f\"{display_name:<18} | {'Low n<3' if len(data_low_std)<3 else 'OK':<25} | {'High n<3' if len(data_high_std)<3 else 'OK':<25} | {'N/A':<10}\")\n",
    "                continue # Skip to next metric if not enough data\n",
    "\n",
    "        # Perform test and print results if data is valid\n",
    "        if data_low is not None and data_high is not None:\n",
    "             try:\n",
    "                 # Check for zero variance which Mann-Whitney U cannot handle\n",
    "                 if np.nanstd(data_low) == 0 or np.nanstd(data_high) == 0:\n",
    "                     print(f\"{display_name:<18} | {'Zero variance data':<25} | {'Zero variance data':<25} | {'N/A':<10}\")\n",
    "                     comparison_results[display_name] = {'p_value': np.nan, 'error': 'Zero variance in data'}\n",
    "                     continue\n",
    "\n",
    "                 stat, p_value = mannwhitneyu(data_low, data_high, alternative='two-sided')\n",
    "                 median_low, q1_low, q3_low = data_low.median(), data_low.quantile(0.25), data_low.quantile(0.75)\n",
    "                 median_high, q1_high, q3_high = data_high.median(), data_high.quantile(0.25), data_high.quantile(0.75)\n",
    "                 low_str = f\"{median_low:.2f} ({q1_low:.2f}-{q3_low:.2f})\"\n",
    "                 high_str = f\"{median_high:.2f} ({q1_high:.2f}-{q3_high:.2f})\"\n",
    "                 p_str = f\"{p_value:.4f}{'*' if p_value < 0.05 else ' '}\"\n",
    "                 print(f\"{display_name:<18} | {low_str:<25} | {high_str:<25} | {p_str:<10}\")\n",
    "                 comparison_results[display_name] = {'p_value': p_value}\n",
    "             except ValueError as ve:\n",
    "                 # Specifically catch cases where all values are identical (leads to ValueError in mannwhitneyu)\n",
    "                 print(f\"{display_name:<18} | {'Test error (e.g., identical values)':<25} | {'Test error':<25} | {'N/A':<10}\")\n",
    "                 comparison_results[display_name] = {'p_value': np.nan, 'error': str(ve)}\n",
    "             except Exception as e:\n",
    "                 print(f\"{display_name:<18} | {'Error during test':<25} | {'Error during test':<25} | {'N/A':<10}\")\n",
    "                 comparison_results[display_name] = {'p_value': np.nan, 'error': str(e)}\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    return comparison_results\n",
    "\n",
    "\n",
    "# --- Main Execution Guard ---\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # == Step 1: Process and Compare Original CSV Data (Parallel) ==\n",
    "    print(\"=\"*80)\n",
    "    print(\" STEP 1: Analyzing and Comparing Original RRI Data (CSV Files) - Parallel \".center(80, \"=\"))\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Find CSV files for each group\n",
    "    original_files_low_csv = sorted([os.path.join(original_rri_dir, f\"{name}_rri.csv\") for name in record_names_low_ahi if os.path.exists(os.path.join(original_rri_dir, f\"{name}_rri.csv\"))])\n",
    "    original_files_high_csv = sorted([os.path.join(original_rri_dir, f\"{name}_rri.csv\") for name in record_names_high_ahi if os.path.exists(os.path.join(original_rri_dir, f\"{name}_rri.csv\"))])\n",
    "\n",
    "    # Calculate HRV for each group from CSVs using parallel processing\n",
    "    hrv_original_low = analyze_group_hrv_csv_parallel(original_files_low_csv, \"Low AHI\", hrv_metrics_nk_names, n_jobs=N_JOBS)\n",
    "    hrv_original_high = analyze_group_hrv_csv_parallel(original_files_high_csv, \"High AHI\", hrv_metrics_nk_names, n_jobs=N_JOBS)\n",
    "\n",
    "    # Perform statistical comparison between groups for original data\n",
    "    results_original = compare_hrv_groups(hrv_original_low, hrv_original_high, metrics_display_map, label_suffix=\"(Original CSV)\")\n",
    "    \n",
    "    print(\"\\n--- Analysis Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899084ac-023d-474c-9f3a-047b8efee4f5",
   "metadata": {},
   "source": [
    "## Statistical tests for RRI of forecast results (tests based on linear regression due to multiple forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b77395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c81bae0-56f7-49fb-bb89-a66cbfbebb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Complete Code for NPY HRV Analysis using Mixed Models ===\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neurokit2 as nk\n",
    "import statsmodels.formula.api as smf # For Mixed Models\n",
    "from scipy.stats import mannwhitneyu # Keep for potential original data comparison later\n",
    "import os\n",
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "predicted_npy_dir = 'apnea_ecg_rri_predicted_samples_chronos_npy'\n",
    "\n",
    "# Define record names for groups (Adjust if your naming convention is different)\n",
    "record_names_low_ahi = [f'c{i:02d}' for i in range(1, 6)] + [f'x{i:02d}' for i in range(1, 31)]\n",
    "record_names_high_ahi = [f'a{i:02d}' for i in range(1, 21)] + [f'b{i:02d}' for i in range(1, 6)]\n",
    "\n",
    "# Metrics to calculate and compare (using NeuroKit2 column names internally)\n",
    "# Make sure these names match the output of nk.hrv() or how you extract them\n",
    "hrv_metrics_nk_names = ['HRV_MeanNN', 'HRV_SDNN', 'HRV_RMSSD', 'HRV_DFA_alpha1']\n",
    "\n",
    "# Mapping to user-friendly names for reporting results\n",
    "metrics_display_map = {\n",
    "    'HRV_MeanNN': 'Mean RRI (ms)',\n",
    "    'HRV_SDNN': 'SDNN (ms)',\n",
    "    'HRV_RMSSD': 'RMSSD (ms)',\n",
    "    'HRV_DFA_alpha1': 'DFA Alpha 1'\n",
    "    # 'Mean HR (bpm)': 'Mean HR (bpm)' # Calculated separately if needed\n",
    "}\n",
    "\n",
    "# Number of parallel jobs (-1 uses all available cores)\n",
    "N_JOBS = -1\n",
    "\n",
    "# --- Ignore Warnings ---\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning, module='neurokit2.*')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='pandas.*')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='statsmodels.*') # Ignore statsmodels warnings if needed\n",
    "\n",
    "# --- HRV Calculation Function (Assumes input is RRI in ms) ---\n",
    "def calculate_hrv_metrics(rri_ms, metrics_to_extract):\n",
    "    \"\"\"Calculates specified HRV metrics using NeuroKit2 from RRI series.\"\"\"\n",
    "    if not isinstance(rri_ms, np.ndarray):\n",
    "        rri_ms = np.array(rri_ms)\n",
    "    # Basic validation of RRI series\n",
    "    if rri_ms is None or len(rri_ms) < 50: # Need sufficient length for reliable HRV\n",
    "        # print(\"Debug: RRI series too short or None.\")\n",
    "        return None\n",
    "    if np.any(rri_ms <= 0) or np.nanstd(rri_ms) == 0: # Check for invalid values or zero variance\n",
    "        # print(\"Debug: RRI series contains non-positive values or has zero standard deviation.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Use NeuroKit2 to calculate HRV indices.\n",
    "        peaks_indices = nk.intervals_to_peaks(rri_ms.flatten()) # Convert intervals back to peak locations if hrv needs it\n",
    "        hrv_indices = nk.hrv(peaks_indices, sampling_rate=100) # Use sampling_rate of the *original* ECG if using peaks\n",
    "\n",
    "        # Extract requested metrics from the resulting DataFrame\n",
    "        metrics = {}\n",
    "        if not isinstance(hrv_indices, pd.DataFrame) or hrv_indices.empty:\n",
    "            # print(\"Debug: nk.hrv did not return a valid DataFrame.\")\n",
    "            return None\n",
    "\n",
    "        # Extract values safely using .get() and accessing the first row .iloc[0]\n",
    "        for nk_name in metrics_to_extract:\n",
    "            # Check if column exists before trying to get value\n",
    "            if nk_name in hrv_indices.columns:\n",
    "                value = hrv_indices[nk_name].iloc[0]\n",
    "                if not pd.isna(value): # Only store non-NaN values\n",
    "                     metrics[nk_name] = value\n",
    "            else:\n",
    "                print(f\"Debug: Metric {nk_name} not found in hrv_indices columns.\")\n",
    "\n",
    "        # Return the dictionary of calculated metrics, or None if empty\n",
    "        return metrics if metrics else None\n",
    "\n",
    "    except Exception as e:\n",
    "        # More specific error logging can be helpful\n",
    "        # print(f\"Debug: HRV calculation failed - {type(e).__name__}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Worker Function for NPY Processing (Returns all samples) ---\n",
    "def process_single_npy_file_all_samples(fpath, metrics_to_extract):\n",
    "    \"\"\"Processes a single NPY file, calculates HRV per sample, returns all results as a list of dicts.\"\"\"\n",
    "    record_name = os.path.basename(fpath).replace('_rri_predicted_samples.npy', '')\n",
    "    try:\n",
    "        predicted_samples = np.load(fpath) # Shape expected: (n_samples, n_timesteps) e.g., (100, 3647)\n",
    "        # Validate shape and content\n",
    "        if predicted_samples.ndim != 2 or predicted_samples.shape[0] == 0 or predicted_samples.shape[1] < 50:\n",
    "            # print(f\"  Skipping {record_name} (NPY): Invalid data shape {predicted_samples.shape} or length < 50.\")\n",
    "            return record_name, None\n",
    "        num_samples = predicted_samples.shape[0]\n",
    "\n",
    "        sample_hrv_metrics_list = []\n",
    "        valid_sample_count = 0\n",
    "        for i in range(num_samples): # Loop through each predicted sample row\n",
    "            rri_sample_ms = predicted_samples[i, :]\n",
    "            metrics = calculate_hrv_metrics(rri_sample_ms, metrics_to_extract)\n",
    "            if metrics is not None:\n",
    "                # Add identifying information to each sample's metrics\n",
    "                metrics['sample_index'] = i\n",
    "                metrics['record_name'] = record_name # Crucial for grouping in mixed model\n",
    "                sample_hrv_metrics_list.append(metrics)\n",
    "                valid_sample_count += 1\n",
    "\n",
    "        # Optional: Add a check for minimum valid samples if desired, but generally return all valid ones\n",
    "        # min_valid_samples_threshold = 10 # Example: require at least 10 valid samples out of 100\n",
    "        # if valid_sample_count < min_valid_samples_threshold:\n",
    "        #     print(f\"  Skipping {record_name} (NPY): Insufficient valid samples ({valid_sample_count}/{num_samples}).\")\n",
    "        #     return record_name, None\n",
    "\n",
    "        if sample_hrv_metrics_list:\n",
    "            # Return the list of dictionaries, one for each valid sample's HRV\n",
    "            return record_name, sample_hrv_metrics_list\n",
    "        else:\n",
    "            # print(f\"  Skipping {record_name} (NPY): No valid HRV metrics calculated for any sample.\")\n",
    "            return record_name, None\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  Warning: NPY File not found {fpath}\")\n",
    "        return record_name, None\n",
    "    except Exception as e:\n",
    "        print(f\"  Skipping {record_name} (NPY): Error during processing - {type(e).__name__}: {e}\")\n",
    "        return record_name, None\n",
    "\n",
    "# --- Function to Analyze Group HRV from NPY files (Collects all samples) ---\n",
    "def analyze_group_hrv_npy_all_samples_parallel(file_list, group_label, metrics_to_extract, n_jobs):\n",
    "    \"\"\"Loads predicted RRI NPY files, calculates HRV for all samples in parallel, returns a single DataFrame.\"\"\"\n",
    "    print(f\"\\nAnalyzing HRV for group: {group_label} (Predicted NPY data - All Samples) using {n_jobs if n_jobs > 0 else 'all'} cores\")\n",
    "    all_samples_hrv_results = [] # Stores dicts from all samples across all files\n",
    "    if not file_list:\n",
    "        print(f\"  No NPY files found for group {group_label}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Parallel processing of each NPY file\n",
    "    # results is a list of tuples: (record_name, list_of_sample_metrics_or_None)\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=10)(\n",
    "        delayed(process_single_npy_file_all_samples)(fpath, metrics_to_extract) for fpath in file_list\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print(f\"  Parallel processing took {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    # Collect and flatten results into a single list of dictionaries\n",
    "    processed_records_count = 0\n",
    "    total_samples_count = 0\n",
    "    for result in results:\n",
    "        if result is not None:\n",
    "            record_name, sample_metrics_list = result\n",
    "            # Check if sample_metrics_list is a non-empty list\n",
    "            if sample_metrics_list is not None and isinstance(sample_metrics_list, list) and sample_metrics_list:\n",
    "                all_samples_hrv_results.extend(sample_metrics_list) # Add list of dicts\n",
    "                processed_records_count += 1\n",
    "                total_samples_count += len(sample_metrics_list)\n",
    "\n",
    "    if all_samples_hrv_results:\n",
    "        # Create a single DataFrame from the list of all sample metrics\n",
    "        hrv_df = pd.DataFrame(all_samples_hrv_results)\n",
    "        # Add the group label column\n",
    "        hrv_df['group'] = group_label\n",
    "        # Ensure all expected columns exist and set standard order\n",
    "        expected_cols = ['record_name', 'group', 'sample_index'] + metrics_to_extract\n",
    "        for col in expected_cols:\n",
    "            if col not in hrv_df.columns:\n",
    "                hrv_df[col] = np.nan # Add missing columns with NaN\n",
    "        hrv_df = hrv_df[expected_cols] # Enforce column order\n",
    "        print(f\"  Successfully processed {processed_records_count} records, collecting {total_samples_count} total samples for {group_label} (NPY).\")\n",
    "        return hrv_df\n",
    "    else:\n",
    "        print(f\"  No valid HRV results could be calculated for {group_label} (NPY - All Samples).\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# --- Function to Compare Groups using Mixed Effects Models ---\n",
    "def compare_hrv_groups_mixed_model(df_all_samples, metrics_map, label_suffix=\"\"):\n",
    "    \"\"\"Performs Linear Mixed Model comparisons for specified HRV metrics.\"\"\"\n",
    "    print(f\"\\n--- Statistical Comparison: Low AHI vs High AHI {label_suffix} (Mixed Model) ---\")\n",
    "    # Basic validation of the combined dataframe\n",
    "    if not isinstance(df_all_samples, pd.DataFrame) or df_all_samples.empty:\n",
    "        print(\"Comparison skipped: Input dataframe is empty or invalid.\")\n",
    "        return None\n",
    "    if 'group' not in df_all_samples.columns or 'record_name' not in df_all_samples.columns:\n",
    "         print(\"Comparison skipped: DataFrame must contain 'group' and 'record_name' columns.\")\n",
    "         return None\n",
    "    if df_all_samples['group'].nunique() < 2:\n",
    "         print(\"Comparison skipped: Need at least two groups in the 'group' column.\")\n",
    "         return None\n",
    "\n",
    "    comparison_results = {}\n",
    "    # Ensure 'group' is treated as a categorical variable\n",
    "    df_all_samples['group'] = pd.Categorical(df_all_samples['group'])\n",
    "    # Optional: Define reference group explicitly if needed, e.g., 'High AHI'\n",
    "    # df_all_samples['group'] = df_all_samples['group'].cat.set_categories(['High AHI', 'Low AHI'])\n",
    "\n",
    "    print(f\"Comparing Low AHI vs High AHI using Linear Mixed Models.\")\n",
    "    print(f\"Total samples analyzed: {len(df_all_samples)}\")\n",
    "    print(f\"Unique records (subjects): {df_all_samples['record_name'].nunique()}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Metric':<18} | {'Group Effect (Low vs High)':<25} | {'Std. Error':<15} | {'z-value':<10} | {'p-value':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Iterate through metrics defined in the display map\n",
    "    for nk_name, display_name in metrics_map.items():\n",
    "         # Check if the metric column exists in the DataFrame\n",
    "         if nk_name not in df_all_samples.columns:\n",
    "             print(f\"{display_name:<18} | {'Metric column missing':<25} | {'N/A':<15} | {'N/A':<10} | {'N/A':<10}\")\n",
    "             continue\n",
    "\n",
    "         # Prepare data for the specific metric: drop rows where the metric, group, or record_name is NaN\n",
    "         metric_data = df_all_samples[[nk_name, 'group', 'record_name']].dropna()\n",
    "\n",
    "         # Check if enough data remains after dropping NaNs\n",
    "         if len(metric_data) < 10 or metric_data['record_name'].nunique() < 2 or metric_data['group'].nunique() < 2:\n",
    "              print(f\"{display_name:<18} | {'Insufficient valid data':<25} | {'N/A':<15} | {'N/A':<10} | {'N/A':<10}\")\n",
    "              continue\n",
    "\n",
    "         try:\n",
    "             # Define the Linear Mixed Model formula:\n",
    "             # Dependent_Variable ~ Fixed_Effect + (Random_Intercept | Grouping_Variable)\n",
    "             # Here: HRV_Metric ~ Group + (1 | Subject_ID)\n",
    "             # C(group, Treatment(reference='High AHI')) ensures 'group' is categorical\n",
    "             # and sets 'High AHI' as the baseline for comparison.\n",
    "             formula = f\"{nk_name} ~ C(group, Treatment(reference='High AHI'))\"\n",
    "\n",
    "             # Define and fit the model\n",
    "             # 'groups=metric_data[\"record_name\"]' specifies the random intercepts for each subject\n",
    "             model = smf.mixedlm(formula, metric_data, groups=metric_data[\"record_name\"])\n",
    "             result = model.fit(reml=False) # Use Maximum Likelihood (ML) for potentially comparing models\n",
    "\n",
    "             # Extract results for the fixed effect of the group (Low AHI vs High AHI)\n",
    "             # Find the coefficient corresponding to the 'Low AHI' level automatically\n",
    "             low_ahi_coef_name = None\n",
    "             for idx in result.summary().tables[1].index:\n",
    "                 # Look for the coefficient name generated by statsmodels for the Low AHI level\n",
    "                 if 'C(group, Treatment(reference=\\'High AHI\\'))[T.Low AHI]' in idx or ('group' in idx and 'Low AHI' in idx):\n",
    "                     low_ahi_coef_name = idx\n",
    "                     break\n",
    "\n",
    "             if low_ahi_coef_name:\n",
    "                 coef = result.params[low_ahi_coef_name]\n",
    "                 stderr = result.bse[low_ahi_coef_name]\n",
    "                 zvalue = result.tvalues[low_ahi_coef_name] # It's z-value for LMM\n",
    "                 pvalue = result.pvalues[low_ahi_coef_name]\n",
    "\n",
    "                 # Format results for printing\n",
    "                 coef_str = f\"{coef:.3f}\"\n",
    "                 stderr_str = f\"{stderr:.3f}\"\n",
    "                 z_str = f\"{zvalue:.2f}\"\n",
    "                 p_str = f\"{pvalue:.4f}{'*' if pvalue < 0.05 else ' '}\" # Add '*' if p < 0.05\n",
    "\n",
    "                 print(f\"{display_name:<18} | {coef_str:<25} | {stderr_str:<15} | {z_str:<10} | {p_str:<10}\")\n",
    "                 comparison_results[display_name] = {'coef': coef, 'stderr': stderr, 'zvalue': zvalue, 'p_value': pvalue}\n",
    "             else:\n",
    "                 print(f\"{display_name:<18} | {'Low AHI effect N/F':<25} | {'N/A':<15} | {'N/A':<10} | {'N/A':<10}\")\n",
    "                 # print(result.summary()) # Print full summary for debugging if coefficient not found\n",
    "\n",
    "         except np.linalg.LinAlgError:\n",
    "             # Handle cases where the model fitting fails due to numerical issues (e.g., singular matrix)\n",
    "              print(f\"{display_name:<18} | {'Model fit failed (LinAlgError)':<25} | {'N/A':<15} | {'N/A':<10} | {'N/A':<10}\")\n",
    "              comparison_results[display_name] = {'p_value': np.nan, 'error': 'LinAlgError'}\n",
    "         except Exception as e:\n",
    "             # Catch other potential errors during model fitting\n",
    "             print(f\"{display_name:<18} | {'Model fit failed':<25} | {type(e).__name__:<15} | {'N/A':<10} | {'N/A':<10}\")\n",
    "             comparison_results[display_name] = {'p_value': np.nan, 'error': str(e)}\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    return comparison_results\n",
    "\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\" Starting NPY HRV Analysis using All Samples and Mixed Models \".center(80, \"=\"))\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # === Step 1: Find NPY Files for Each Group ===\n",
    "    predicted_files_low_npy = []\n",
    "    for name in record_names_low_ahi:\n",
    "        fpath = os.path.join(predicted_npy_dir, f\"{name}_rri_predicted_samples.npy\")\n",
    "        if os.path.exists(fpath):\n",
    "            predicted_files_low_npy.append(fpath)\n",
    "    print(f\"Found {len(predicted_files_low_npy)} NPY files for Low AHI group.\")\n",
    "\n",
    "    predicted_files_high_npy = []\n",
    "    for name in record_names_high_ahi:\n",
    "         fpath = os.path.join(predicted_npy_dir, f\"{name}_rri_predicted_samples.npy\")\n",
    "         if os.path.exists(fpath):\n",
    "             predicted_files_high_npy.append(fpath)\n",
    "    print(f\"Found {len(predicted_files_high_npy)} NPY files for High AHI group.\")\n",
    "\n",
    "    if not predicted_files_low_npy and not predicted_files_high_npy:\n",
    "        print(\"\\nError: No NPY files found in the specified directory for either group. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # === Step 2: Calculate HRV for All Samples from NPY Files (Parallel) ===\n",
    "    hrv_predicted_low_all = analyze_group_hrv_npy_all_samples_parallel(\n",
    "        predicted_files_low_npy, \"Low AHI\", hrv_metrics_nk_names, n_jobs=N_JOBS\n",
    "    )\n",
    "    hrv_predicted_high_all = analyze_group_hrv_npy_all_samples_parallel(\n",
    "        predicted_files_high_npy, \"High AHI\", hrv_metrics_nk_names, n_jobs=N_JOBS\n",
    "    )\n",
    "\n",
    "    # === Step 3: Combine Data and Perform Mixed Model Comparison ===\n",
    "    hrv_predicted_all_samples = pd.DataFrame() # Initialize empty DataFrame\n",
    "    # Check if both DataFrames are not empty before concatenating\n",
    "    if isinstance(hrv_predicted_low_all, pd.DataFrame) and not hrv_predicted_low_all.empty and \\\n",
    "       isinstance(hrv_predicted_high_all, pd.DataFrame) and not hrv_predicted_high_all.empty:\n",
    "        hrv_predicted_all_samples = pd.concat([hrv_predicted_low_all, hrv_predicted_high_all], ignore_index=True)\n",
    "        print(f\"\\nCombined DataFrame created with {len(hrv_predicted_all_samples)} total samples.\")\n",
    "\n",
    "        # Perform the statistical comparison using the mixed model function\n",
    "        results_predicted_mixed = compare_hrv_groups_mixed_model(\n",
    "            hrv_predicted_all_samples,\n",
    "            metrics_display_map, # Pass the map for display names\n",
    "            label_suffix=\"(Predicted NPY - All Samples)\"\n",
    "        )\n",
    "    elif isinstance(hrv_predicted_low_all, pd.DataFrame) and not hrv_predicted_low_all.empty:\n",
    "        print(\"\\nWarning: Data found only for Low AHI group. Cannot perform comparison.\")\n",
    "        # Optionally, you could analyze the single group data here if needed\n",
    "        hrv_predicted_all_samples = hrv_predicted_low_all\n",
    "    elif isinstance(hrv_predicted_high_all, pd.DataFrame) and not hrv_predicted_high_all.empty:\n",
    "        print(\"\\nWarning: Data found only for High AHI group. Cannot perform comparison.\")\n",
    "        hrv_predicted_all_samples = hrv_predicted_high_all\n",
    "    else:\n",
    "        print(\"\\nError: Failed to gather HRV data from NPY files for both groups. Cannot proceed with comparison.\")\n",
    "\n",
    "    # === Step 4: (Optional) Save the combined DataFrame ===\n",
    "    # if not hrv_predicted_all_samples.empty:\n",
    "    #     try:\n",
    "    #         save_path = \"predicted_hrv_all_samples.csv\"\n",
    "    #         hrv_predicted_all_samples.to_csv(save_path, index=False)\n",
    "    #         print(f\"\\nCombined HRV data saved to {save_path}\")\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"\\nError saving combined data: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" Analysis Finished \".center(80, \"=\"))\n",
    "    print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
